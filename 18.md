# 1 Introduction and flat files

In this chapter, you'll learn how to import data into Python from all types of flat files, which are a simple and prevalent form of data storage. You've previously learned how to use NumPy and pandas—you will learn how to use these packages to import flat files and customize your imports.

## 1.1 Welcome to the course!

## 1.2 Exploring your working directory

In order to import data into Python, you should first have an idea of what files are in your working directory.

IPython, which is running on DataCamp's servers, has a bunch of cool commands, including its [magic commands](https://ipython.readthedocs.io/en/stable/overview.html). For example, starting a line with `!` gives you complete system shell access. This means that the IPython magic command `! ls` will display the contents of your current directory. Your task is to use the IPython magic command `! ls` to check out the contents of your current directory and answer the following question: which of the following files is in your working directory?

### 1.2.1 Instructions

### Possible Answers

- [ ] `huck_finn.txt`

- [ ] `titanic.csv`

- [x] `moby_dick.txt`

## 1.3 Importing entire text files

In this exercise, you'll be working with the file `moby_dick.txt`. It is a text file that contains the opening sentences of Moby Dick, one of the great American novels! Here you'll get experience opening a text file, printing its contents to the shell and, finally, closing it.

### 1.3.1 Instructions

- Open the file `moby_dick.txt` as *read-only* and store it in the variable `file`. Make sure to pass the filename enclosed in quotation marks `''`.
- Print the contents of the file to the shell using the `print()` function. As Hugo showed in the video, you'll need to apply the method `read()` to the object `file`.
- Check whether the file is closed by executing `print(file.closed)`.
- Close the file using the `close()` method.
- Check again that the file is closed as you did above.

## 1.4 Importing text files line by line

For large files, we may not want to print all of their content to the shell: you may wish to print only the first few lines. Enter the `readline()` method, which allows you to do this. When a file called `file` is open, you can print out the first line by executing `file.readline()`. If you execute the same command again, the second line will print, and so on.

In the introductory video, Hugo also introduced the concept of a **context manager**. He showed that you can bind a variable `file` by using a context manager construct:

```python
with open('huck_finn.txt') as file:
```

While still within this construct, the variable `file` will be bound to `open('huck_finn.txt')`; thus, to print the file to the shell, all the code you need to execute is:

```python
with open('huck_finn.txt') as file:
    print(file.readline())
```

You'll now use these tools to print the first few lines of `moby_dick.txt`!

### 1.4.1 Instructions

- Open `moby_dick.txt` using the `with` context manager and the variable `file`.
- Print the first three lines of the file to the shell by using `readline()` three times within the context manager.

## 1.5 The importance of flat files in data science

## 1.6 Pop quiz: examples of flat files

You're now well-versed in importing text files and you're about to become a wiz at importing flat files. But can you remember exactly what a flat file is? Test your knowledge by answering the following question: which of these file types below is NOT an example of a flat file?

### 1.6.1 Answer the question

### Possible Answers

- [ ] A .csv file.

- [ ] A tab-delimited .txt.

- [x] A relational database (e.g. PostgreSQL).

## 1.7 Pop quiz: what exactly are flat files?

Which of the following statements about flat files is incorrect?

### 1.7.1 Answer the question

### Possible Answers

- [ ] Flat files consist of rows and each row is called a record.

- [x] Flat files consist of multiple tables with structured relationships between the tables.

- [ ] A record in a flat file is composed of *fields* or *attributes*, each of which contains at most one item of information.

- [ ] Flat files are pervasive in data science.

## 1.8 Why we like flat files and the Zen of Python

In PythonLand, there are currently hundreds of *Python Enhancement Proposals*, commonly referred to as *PEP*s. [PEP8](https://www.python.org/dev/peps/pep-0008/), for example, is a standard style guide for Python, written by our sensei Guido van Rossum himself. It is the basis for how we here at DataCamp ask our instructors to style their code. Another one of my favorites is [PEP20](https://www.python.org/dev/peps/pep-0020/), commonly called the *Zen of Python*. Its abstract is as follows:

> Long time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, only 19 of which have been written down.

If you don't know what the acronym `BDFL` stands for, I suggest that you look [here](https://docs.python.org/3.3/glossary.html#term-bdfl). You can print the Zen of Python in your shell by typing `import this` into it! You're going to do this now and the 5th aphorism (line) will say something of particular interest.

The question you need to answer is: **what is the 5th aphorism of the *Zen of Python*?**

### 1.8.1 Instructions

### Possible Answers

- [x] Flat is better than nested.

- [ ] Flat files are essential for data science.

- [ ] The world is representable as a flat file.

- [ ] Flatness is in the eye of the beholder.

## 1.9 Importing flat files using NumPy

## 1.10 Using NumPy to import flat files

In this exercise, you're now going to load the MNIST digit recognition dataset using the numpy function `loadtxt()` and see just how easy it can be:

- The first argument will be the filename.
- The second will be the delimiter which, in this case, is a comma.

You can find more information about the MNIST dataset [here](http://yann.lecun.com/exdb/mnist/) on the webpage of Yann LeCun, who is currently Director of AI Research at Facebook and Founding Director of the NYU Center for Data Science, among many other things.

### 1.10.1 Instructions

- Fill in the arguments of `np.loadtxt()` by passing `file` and a comma `','` for the delimiter.
- Fill in the argument of `print()` to print the type of the object `digits`. Use the function `type()`.
- Execute the rest of the code to visualize one of the rows of the data.

## 1.11 Customizing your NumPy import

What if there are rows, such as a header, that you don't want to import? What if your file has a delimiter other than a comma? What if you only wish to import particular columns?

There are a number of arguments that `np.loadtxt()` takes that you'll find useful:

- `delimiter` changes the delimiter that `loadtxt()` is expecting.
  - You can use `','` for comma-delimited.
  - You can use `'\t'` for tab-delimited.
- `skiprows` allows you to specify *how many rows* (not indices) you wish to skip
- `usecols` takes a *list* of the indices of the columns you wish to keep.

The file that you'll be importing, `digits_header.txt`, has a header and is tab-delimited.

### 1.11.1 Instructions

- Complete the arguments of `np.loadtxt()`: the file you're importing is tab-delimited, you want to skip the first row and you only want to import the first and third columns.
- Complete the argument of the `print()` call in order to print the entire array that you just imported.

## 1.12 Importing different datatypes

The file `seaslug.txt`

- has a text header, consisting of strings
- is tab-delimited.

These data consists of percentage of sea slug larvae that had metamorphosed in a given time period. Read more [here](http://www.stat.ucla.edu/~rgould/datasets/aboutseaslugs.html).

Due to the header, if you tried to import it as-is using `np.loadtxt()`, Python would throw you a `ValueError` and tell you that it `could not convert string to float`. There are two ways to deal with this: firstly, you can set the data type argument `dtype` equal to `str` (for string).

Alternatively, you can skip the first row as we have seen before, using the `skiprows` argument.

### 1.12.1 Instructions

- Complete the first call to `np.loadtxt()` by passing `file` as the first argument.
- Execute `print(data[0])` to print the first element of `data`.
- Complete the second call to `np.loadtxt()`. The file you're importing is tab-delimited, the datatype is `float`, and you want to skip the first row.
- Print the 10th element of `data_float` by completing the `print()` command. Be guided by the previous `print()` call.
- Execute the rest of the code to visualize the data.

### 1.13 Working with mixed datatypes (1)

Much of the time you will need to import datasets which have different datatypes in different columns; one column may contain strings and another floats, for example. The function `np.loadtxt()` will freak at this. There is another function, `np.genfromtxt()`, which can handle such structures. If we pass `dtype=None` to it, it will figure out what types each column should be.

Import `'titanic.csv'` using the function `np.genfromtxt()` as follows:

```python
data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)
```

Here, the first argument is the filename, the second specifies the delimiter `,` and the third argument `names` tells us there is a header. Because the data are of different types, `data` is an object called a [structured array](http://docs.scipy.org/doc/numpy/user/basics.rec.html). Because numpy arrays have to contain elements that are all the same type, the structured array solves this by being a 1D array, where each element of the array is a row of the flat file imported. You can test this by checking out the array's shape in the shell by executing `np.shape(data)`.

Accessing rows and columns of structured arrays is super-intuitive: to get the ith row, merely execute `data[i]` and to get the column with name `'Fare'`, execute `data['Fare']`.

After importing the Titanic data as a structured array (as per the instructions above), print the entire column with the name `Survived` to the shell. What are the last 4 values of this column?

### 1.13.1 Instructions

### Possible Answers

- [ ] 1,0,0,1.

- [ ] 1,2,0,0.

- [x] 1,0,1,0.

- [ ] 0,1,1,1.

## 1.14 Working with mixed datatypes (2)

You have just used `np.genfromtxt()` to import data containing mixed datatypes. There is also another function `np.recfromcsv()` that behaves similarly to `np.genfromtxt()`, except that its default `dtype` is `None`. In this exercise, you'll practice using this to achieve the same result.

### 1.14.1 Instructions

- Import `titanic.csv` using the function `np.recfromcsv()` and assign it to the variable, `d`. You'll only need to pass `file` to it because it has the defaults `delimiter=','` and `names=True` in addition to `dtype=None`!
- Run the remaining code to print the first three entries of the resulting array `d`.

## 1.15 Importing flat files using pandas

## 1.16 Using pandas to import flat files as DataFrames (1)

In the last exercise, you were able to import flat files containing columns with different datatypes as `numpy` arrays. However, the `DataFrame` object in pandas is a more appropriate structure in which to store such data and, thankfully, we can easily import files of mixed data types as DataFrames using the pandas functions `read_csv()` and `read_table()`.

### 1.16.1 Instructions

- Import the `pandas` package using the alias `pd`.
- Read `titanic.csv` into a DataFrame called `df`. The file name is already stored in the `file` object.
- In a `print()` call, view the head of the DataFrame.

## 1.17 Using pandas to import flat files as DataFrames (2)

In the last exercise, you were able to import flat files into a `pandas` DataFrame. As a bonus, it is then straightforward to retrieve the corresponding `numpy` array using the attribute `values`. You'll now have a chance to do this using the MNIST dataset, which is available as `digits.csv`.

### 1.17.1 Instructions

- Import the first 5 rows of the file into a DataFrame using the function `pd.read_csv()` and assign the result to `data`. You'll need to use the arguments `nrows` and `header` (there is no header in this file).
- Build a `numpy` array from the resulting DataFrame in `data` and assign to `data_array`.
- Execute `print(type(data_array))` to print the datatype of `data_array`.

## 1.18 Customizing your pandas import

The `pandas` package is also great at dealing with many of the issues you will encounter when importing data as a data scientist, such as comments occurring in flat files, empty lines and missing values. Note that missing values are also commonly referred to as `NA` or `NaN`. To wrap up this chapter, you're now going to import a slightly corrupted copy of the Titanic dataset `titanic_corrupt.txt`, which

- contains comments after the character `'#'`
- is tab-delimited.

### 1.18.1 Instructions

- Complete the `sep` (the `pandas` version of `delim`), `comment` and `na_values` arguments of `pd.read_csv()`. `comment` takes characters that comments occur after in the file, which in this case is `'#'`. `na_values` takes a list of strings to recognize as `NA`/`NaN`, in this case the string `'Nothing'`.
- Execute the rest of the code to print the head of the resulting DataFrame and plot the histogram of the `'Age'` of passengers aboard the Titanic.

## 1.19 Final thoughts on data import

# 2 Importing data from other file types

You've learned how to import flat files, but there are many other file types you will potentially have to work with as a data scientist. In this chapter, you'll learn how to import data into Python from a wide array of important file types. These include pickled files, Excel spreadsheets, SAS and Stata files, HDF5 files, a file type for storing large quantities of numerical data, and MATLAB files.

## 2.1 Introduction to other file types

## 2.2 Not so flat any more

In Chapter 1, you learned how to use the IPython magic command `! ls` to explore your current working directory. You can also do this natively in Python using the [library `os`](https://docs.python.org/2/library/os.html), which consists of miscellaneous operating system interfaces.

The first line of the following code imports the library `os`, the second line stores the name of the current directory in a string called `wd` and the third outputs the contents of the directory in a list to the shell.

```python
import os
wd = os.getcwd()
os.listdir(wd)
```

Run this code in the IPython shell and answer the following questions. Ignore the files that begin with `.`.

Check out the contents of your current directory and answer the following questions: (1) which file is in your directory and NOT an example of a flat file; (2) why is it not a flat file?

### 2.2.1 Instructions

### Possible Answers

- [ ] `database.db` is not a flat file because relational databases contain structured relationships and flat files do not.

- [x] `battledeath.xlsx` is not a flat because it is a spreadsheet consisting of many sheets, not a single table.

- [ ] `titanic.txt` is not a flat file because it is a `.txt`, not a `.csv`.

## 2.3 Loading a pickled file

There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.

However, if you merely want to be able to import them into Python, you can [serialize](https://en.wikipedia.org/wiki/Serialization) them. All this means is converting the object into a sequence of bytes, or a bytestream.

In this exercise, you'll import the `pickle` package, open a previously pickled data structure from a file and load it.

### 2.3.1 Instructions

- Import the `pickle` package.
- Complete the second argument of `open()` so that it is read only for a binary file. This argument will be a string of two letters, one signifying 'read only', the other 'binary'.
- Pass the correct argument to `pickle.load()`; it should use the variable that is bound to `open`.
- Print the data, `d`.
- Print the datatype of `d`; take your mind back to your previous use of the function `type()`.

## 2.4 Listing sheets in Excel files

Whether you like it or not, any working data scientist will need to deal with Excel spreadsheets at some point in time. You won't always want to do so in Excel, however!

Here, you'll learn how to use `pandas` to import Excel spreadsheets and how to list the names of the sheets in any loaded .xlsx file.

Recall from the video that, given an Excel file imported into a variable `spreadsheet`, you can retrieve a list of the sheet names using the attribute `spreadsheet.sheet_names`.

Specifically, you'll be loading and checking out the spreadsheet `'battledeath.xlsx'`, modified from the Peace Research Institute Oslo's (PRIO) [dataset](https://www.prio.org/Data/Armed-Conflict/Battle-Deaths/The-Battle-Deaths-Dataset-version-30/). This data contains age-adjusted mortality rates due to war in various countries over several years.

### 2.4.1 Instructions

- Assign the spreadsheet filename (provided above) to the variable `file`.
- Pass the correct argument to `pd.ExcelFile()` to load the file using pandas, assigning the result to the variable `xls`.
- Print the sheetnames of the Excel spreadsheet by passing the necessary argument to the `print()` function.

## 2.5 Importing sheets from Excel files

In the previous exercises, you saw that the Excel file contains two sheets, `'2002'` and `'2004'`. The next step is to import these.

In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.

The spreadsheet `'battledeath.xlsx'` is already loaded as `xls`.

### 2.5.1 Instructions

- Load the sheet `'2004'` into the DataFrame `df1` using its name as a string.
- Print the head of `df1` to the shell.
- Load the sheet `2002` into the DataFrame `df2` using its index (`0`).
- Print the head of `df2` to the shell.

## 2.6 Customizing your spreadsheet import

Here, you'll parse your spreadsheets and use additional arguments to skip rows, rename columns and select only particular columns.

The spreadsheet `'battledeath.xlsx'` is already loaded as `xls`.

As before, you'll use the method `parse()`. This time, however, you'll add the additional arguments `skiprows`, `names` and `usecols`. These skip rows, name the columns and designate which columns to parse, respectively. All these arguments can be assigned to lists containing the specific row numbers, strings and column numbers, as appropriate.

### 2.6.1 Instructions

- Parse the first sheet by index. In doing so, skip the first row of data and name the columns `'Country'` and `'AAM due to War (2002)'` using the argument `names`. The values passed to `skiprows` and `names` all need to be of type `list`.
- Parse the second sheet by index. In doing so, parse only the first column with the `usecols` parameter, skip the first row and rename the column `'Country'`. The argument passed to `usecols` also needs to be of type `list`.

## 2.7 Importing SAS/Stata files using pandas

## 2.8 How to import SAS7BDAT

How do you correctly import the function `SAS7BDAT()` from the package sas7bdat?

### 2.8.1 Answer the question

### Possible Answers

- [ ] `import SAS7BDAT from sas7bdat`

- [ ] `from SAS7BDAT import sas7bdat`

- [ ] `import sas7bdat from SAS7BDAT`

- [x] `from sas7bdat import SAS7BDAT`

## 2.9 Importing SAS files

In this exercise, you'll figure out how to import a SAS file as a DataFrame using `SAS7BDAT` and `pandas`. The file `'sales.sas7bdat'` is already in your working directory and both `pandas` and `matplotlib.pyplot` have already been imported as follows:

```python
import pandas as pd
import matplotlib.pyplot as plt
```

The data are adapted from the website of the undergraduate text book [Principles of Econometrics](http://www.principlesofeconometrics.com/) by Hill, Griffiths and Lim.

### 2.9.1 Instructions

- Import the module `SAS7BDAT` from the library `sas7bdat`.
- In the context of the file `'sales.sas7bdat'`, load its contents to a DataFrame `df_sas`, using the method `to_data_frame()` on the object `file`.
- Print the head of the DataFrame `df_sas`.
- Execute your entire script to produce a histogram plot!

## 2.10 Using read_stata to import Stata files

The `pandas` package has been imported in the environment as `pd` and the file `disarea.dta` is in your working directory. The data consist of disease extents for several diseases in various countries (more information can be found [here](http://www.cid.harvard.edu/ciddata/geog/readme_disarea.html)).

What is the correct way of using the `read_stata()` function to import `disarea.dta` into the object `df`?

### 2.10.1 Instructions

### Possible Answers

- [ ] `df = 'disarea.dta'`

- [ ] `df = read_stata.pd('disarea.dta')`

- [x] `df = pd.read_stata('disarea.dta')`

- [ ] `df = pd.read_stata(disarea.dta)`

## 2.11 Importing Stata files

Here, you'll gain expertise in importing Stata files as DataFrames using the `pd.read_stata()` function from `pandas`. The last exercise's file, `'disarea.dta'`, is still in your working directory.

### 2.11.1 Instructions

- Use `pd.read_stata()` to load the file `'disarea.dta'` into the DataFrame `df`.
- Print the head of the DataFrame `df`.
- Visualize your results by plotting a histogram of the column `disa10`. We’ve already provided this code for you, so just run it!

## 2.12 Importing HDF5 files

## 2.13 Using File to import HDF5 files

The `h5py` package has been imported in the environment and the file `LIGO_data.hdf5` is loaded in the object `h5py_file`.

What is the correct way of using the h5py function, `File()`, to import the file in `h5py_file` into an object, `h5py_data`, for *reading* only?

### 2.13.1 Instructions

### Possible Answers

- [ ] `h5py_data = File(h5py_file, 'r')`

- [x] `h5py_data = h5py.File(h5py_file, 'r')`

- [ ] `h5py_data = h5py.File(h5py_file, read)`

- [ ] `h5py_data = h5py.File(h5py_file, 'read')`

## 2.14 Using h5py to import HDF5 files

The file `'LIGO_data.hdf5'` is already in your working directory. In this exercise, you'll import it using the `h5py` library. You'll also print out its datatype to confirm you have imported it correctly. You'll then study the structure of the file in order to see precisely what HDF groups it contains.

You can find the LIGO data plus loads of documentation and tutorials [here](https://losc.ligo.org/events/GW150914/). There is also a great tutorial on Signal Processing with the data [here](https://www.gw-openscience.org/GW150914data/LOSC_Event_tutorial_GW150914.html).

### 2.14.1 Instructions

- Import the package `h5py`.
- Assign the name of the file to the variable `file`.
- Load the file as read only into the variable `data`.
- Print the datatype of `data`.
- Print the names of the groups in the HDF5 file `'LIGO_data.hdf5'`.

## 2.15 Extracting data from your HDF5 file

In this exercise, you'll extract some of the LIGO experiment's actual data from the HDF5 file and you'll visualize it.

To do so, you'll need to first explore the HDF5 group `'strain'`.

### 2.15.1 Instructions

- Assign the HDF5 group `data['strain']` to `group`.
- In the `for` loop, print out the keys of the HDF5 group in `group`.
- Assign the time series data `data['strain']['Strain']` to a NumPy array called `strain`.
- Set `num_samples` equal to `10000`, the number of time points we wish to sample.
- Execute the rest of the code to produce a plot of the time series data in `LIGO_data.hdf5`.

## 2.16 Importing MATLAB files

## 2.17 Loading .mat files

In this exercise, you'll figure out how to load a MATLAB file using `scipy.io.loadmat()` and you'll discover what Python datatype it yields.

The file `'albeck_gene_expression.mat'` is in your working directory. This file contains [gene expression data](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm) from the Albeck Lab at UC Davis. You can find the data and some great documentation [here](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm).

### 2.17.1 Instructions

- Import the package `scipy.io`.
- Load the file `'albeck_gene_expression.mat'` into the variable `mat`; do so using the function `scipy.io.loadmat()`.
- Use the function `type()` to print the datatype of `mat` to the IPython shell.

## 2.18 The structure of .mat in Python

Here, you'll discover what is in the MATLAB dictionary that you loaded in the previous exercise.

The file `'albeck_gene_expression.mat'` is already loaded into the variable `mat`. The following libraries have already been imported as follows:

```python
import scipy.io
import matplotlib.pyplot as plt
import numpy as np
```

Once again, this file contains [gene expression data](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm) from the Albeck Lab at UCDavis. You can find the data and some great documentation [here](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm).

### 2.18.1 Instructions

- Use the method `.keys()` on the dictionary `mat` to print the keys. Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.
- Print the type of the value corresponding to the key `'CYratioCyt'` in `mat`. Recall that `mat['CYratioCyt']` accesses the value.
- Print the shape of the value corresponding to the key `'CYratioCyt'` using the `numpy` function `shape()`.
- Execute the entire script to see some oscillatory gene expression data!

# 3 Working with relational databases in Python

In this chapter, you'll learn how to extract meaningful data from relational databases, an essential skill for any data scientist. You will learn about relational models, how to create SQL queries, how to filter and order your SQL records, and how to perform advanced queries by joining database tables.

## 3.1 Introduction to relational databases

## 3.2 Pop quiz: The relational model

Which of the following is not part of the relational model?

### 3.2.1 Answer the question

### Possible Answers

- [ ] Each row or record in a table represents an instance of an entity type.

- [ ] Each column in a table represents an attribute or feature of an instance.

- [ ] Every table contains a primary key column, which has a unique entry for each row.

- [x] A database consists of at least 3 tables.

- [ ] There are relations between tables.

## 3.3 Creating a database engine in Python

## 3.4 Creating a database engine

Here, you're going to fire up your very first SQL engine. You'll create an engine to connect to the SQLite database `'Chinook.sqlite'`, which is in your working directory. Remember that to create an engine to connect to `'Northwind.sqlite'`, Hugo executed the command

```python
engine = create_engine('sqlite:///Northwind.sqlite')
```

Here, `'sqlite:///Northwind.sqlite'` is called the *connection string* to the SQLite database `Northwind.sqlite`. A little bit of background on the [Chinook database](https://github.com/lerocha/chinook-database): the Chinook database contains information about a semi-fictional digital media store in which media data is real and customer, employee and sales data has been manually created.

Why the name Chinook, you ask? According to their [website](https://github.com/lerocha/chinook-database),

> The name of this sample database was based on the Northwind database. Chinooks are winds in the interior West of North America, where the Canadian Prairies and Great Plains meet various mountain ranges. Chinooks are most prevalent over southern Alberta in Canada. Chinook is a good name choice for a database that intends to be an alternative to Northwind.

### 3.4.1 Instructions

- Import the function `create_engine` from the module `sqlalchemy`.
- Create an engine to connect to the SQLite database `'Chinook.sqlite'` and assign it to `engine`.

## 3.5 What are the tables in the database?

In this exercise, you'll once again create an engine to connect to `'Chinook.sqlite'`. Before you can get any data out of the database, however, you'll need to know what tables it contains!

To this end, you'll save the table names to a list using the method `table_names()` on the engine and then you will print the list.

### 3.5.1 Instructions

- Import the function `create_engine` from the module `sqlalchemy`.
- Create an engine to connect to the SQLite database `'Chinook.sqlite'` and assign it to `engine`.
- Using the method `table_names()` on the engine `engine`, assign the table names of `'Chinook.sqlite'` to the variable `table_names`.
- Print the object `table_names` to the shell.

## 3.6 Querying relational databases in Python

## 3.7 The Hello World of SQL Queries!

Now, it's time for liftoff! In this exercise, you'll perform the Hello World of SQL queries, `SELECT`, in order to retrieve all columns of the table `Album` in the Chinook database. Recall that the query `SELECT *` selects all columns.

### 3.7.1 Instructions

- Open the engine connection as `con` using the method `connect()` on the engine.
- Execute the query that **selects** *ALL columns* **from** the `Album` table. Store the results in `rs`.
- Store all of your query results in the DataFrame `df` by applying the `fetchall()` method to the results `rs`.
- Close the connection!

## 3.8 Customizing the Hello World of SQL Queries

Congratulations on executing your first SQL query! Now you're going to figure out how to customize your query in order to:

- Select specified columns from a table;
- Select a specified number of rows;
- Import column names from the database table.

Recall that Hugo performed a very similar query customization in the video:

```python
engine = create_engine('sqlite:///Northwind.sqlite')

with engine.connect() as con:
    rs = con.execute("SELECT OrderID, OrderDate, ShipName FROM Orders")
    df = pd.DataFrame(rs.fetchmany(size=5))
    df.columns = rs.keys()
```

Packages have already been imported as follows:

```python
from sqlalchemy import create_engine
import pandas as pd
```

The engine has also already been created:

```python
engine = create_engine('sqlite:///Chinook.sqlite')
```

The engine connection is already open with the statement

```python
with engine.connect() as con:
```

All the code you need to complete is within this context.

### 3.8.1 Instructions

- Execute the SQL query that **selects** the columns `LastName` and `Title` **from** the `Employee` table. Store the results in the variable `rs`.
- Apply the method `fetchmany()` to `rs` in order to retrieve 3 of the records. Store them in the DataFrame `df`.
- Using the `rs` object, set the DataFrame's column names to the corresponding names of the table columns.

## 3.9 Filtering your database records using SQL's WHERE

You can now execute a basic SQL query to select records from any table in your database and you can also perform simple query customizations to select particular columns and numbers of rows.

There are a couple more standard SQL query chops that will aid you in your journey to becoming an SQL ninja.

Let's say, for example that you wanted to get all records from the `Customer` table of the Chinook database for which the `Country` is `'Canada'`. You can do this very easily in SQL using a `SELECT` statement followed by a `WHERE` clause as follows:

```python
SELECT * FROM Customer WHERE Country = 'Canada'
```

In fact, you can filter any `SELECT` statement by any condition using a `WHERE` clause. This is called *filtering* your records.

In this interactive exercise, you'll select all records of the `Employee` table for which `'EmployeeId'` is greater than or equal to `6`.

Packages are already imported as follows:

```python
import pandas as pd
from sqlalchemy import create_engine
```

Query away!

### 3.9.1 Instructions

- Complete the argument of `create_engine()` so that the engine for the SQLite database `'Chinook.sqlite'` is created.
- Execute the query that **selects** *all* records **from** the `Employee` table **where** `'EmployeeId'` is greater than or equal to `6`. Use the `>=` operator and assign the results to `rs`.
- Apply the method `fetchall()` to `rs` in order to fetch all records in `rs`. Store them in the DataFrame `df`.
- Using the `rs` object, set the DataFrame's column names to the corresponding names of the table columns.

## 3.10 Ordering your SQL records with ORDER BY

You can also *order* your SQL query results. For example, if you wanted to get all records from the `Customer` table of the Chinook database and order them in increasing order by the column `SupportRepId`, you could do so with the following query:

```python
"SELECT * FROM Customer ORDER BY SupportRepId"
```

In fact, you can order any `SELECT` statement by any column.

In this interactive exercise, you'll select all records of the `Employee` table and order them in increasing order by the column `BirthDate`.

Packages are already imported as follows:

```python
import pandas as pd
from sqlalchemy import create_engine
```

Get querying!

### 3.10.1 Instructions

- Using the function `create_engine()`, create an engine for the SQLite database `Chinook.sqlite` and assign it to the variable `engine`.
- In the context manager, execute the query that **selects** *all* records **from** the `Employee` table and **orders** them in increasing order **by** the column `BirthDate`. Assign the result to `rs`.
- In a call to `pd.DataFrame()`, apply the method `fetchall()` to `rs` in order to fetch all records in `rs`. Store them in the DataFrame `df`.
- Set the DataFrame's column names to the corresponding names of the table columns.

## 3.11 Querying relational databases directly with pandas

## 3.12 Pandas and The Hello World of SQL Queries!

Here, you'll take advantage of the power of `pandas` to write the results of your SQL query to a DataFrame in one swift line of Python code!

You'll first import `pandas` and create the SQLite `'Chinook.sqlite'` engine. Then you'll query the database to select all records from the `Album` table.

Recall that to select all records from the `Orders` table in the Northwind database, Hugo executed the following command:

```python
df = pd.read_sql_query("SELECT * FROM Orders", engine)
```

### 3.12.1 Instructions

- Import the `pandas` package using the alias `pd`.
- Using the function `create_engine()`, create an engine for the SQLite database `Chinook.sqlite` and assign it to the variable `engine`.
- Use the `pandas` function `read_sql_query()` to assign to the variable `df` the DataFrame of results from the following query: **select** *all* records **from** the table `Album`.
- The remainder of the code is included to confirm that the DataFrame created by this method is equal to that created by the previous method that you learned.

## 3.13 Pandas for more complex querying

Here, you'll become more familiar with the pandas function `read_sql_query()` by using it to execute a more complex query: a `SELECT` statement followed by both a `WHERE` clause AND an `ORDER BY` clause.

You'll build a DataFrame that contains the rows of the `Employee` table for which the `EmployeeId` is greater than or equal to `6` and you'll order these entries by `BirthDate`.

### 3.13.1 Instructions

- Using the function `create_engine()`, create an engine for the SQLite database `Chinook.sqlite` and assign it to the variable `engine`.
- Use the `pandas` function `read_sql_query()` to assign to the variable `df` the DataFrame of results from the following query: **select** *all* records **from** the `Employee` table **where** the `EmployeeId` is greater than or equal to `6` and **ordered by** `BirthDate` (make sure to use `WHERE` and `ORDER BY` in this precise order).

## 3.14 Advanced querying: exploiting table relationships

## 3.15


































































































































































































































































































































































































































































































































































